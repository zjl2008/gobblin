
job.name=GobblinKafkaToHdfs
job.group=GobblinToHdfs1
job.description=Pull data from kafka to hdfs use Gobblin
job.lock.enabled=false
job.schedule=0 0/3 * * * ?
kafka.brokers=url:9092
 
source.class=gobblin.source.extractor.extract.kafka.KafkaSimpleSource
extract.namespace=gobblin.extract.kafka
topic.whitelist=iemp

simple.writer.delimiter=\n
simple.writer.prepend.size=false
writer.partitioner.class=gobblin.writer.partitioner.TimeBasedJsonWriterPartitioner 
writer.partition.level=hourly
writer.partition.pattern=yyyy/MM/dd/HH
writer.partition.columns=time
writer.partition.columns2=pid
writer.partition.timezone=Asia/Shanghai

writer.include.partition.in.file.names=true


writer.builder.class=gobblin.writer.SimpleDataWriterBuilder

writer.file.path.type=tablename

writer.destination.type=HDFS

writer.output.format=txt
data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher
 
mr.job.max.mappers=1
 
metrics.reporting.file.enabled=true
metrics.log.dir=/gobblin-kafka/metrics
metrics.reporting.file.suffix=txt
 
bootstrap.with.offset=earliest
 
fs.uri=hdfs://ip:8020
writer.fs.uri=${fs.uri}
state.store.fs.uri=${fs.uri}
 
mr.job.root.dir=/gobblin-kafka/working
state.store.dir=/gobblin-kafka/state-store
task.data.root.dir=/jobs/kafkaetl/gobblin/gobblin-kafka/task-data
data.publisher.final.dir=/gobblintest/job-output
